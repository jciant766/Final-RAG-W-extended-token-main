# ğŸ“ Malta Legal RAG System - Project Structure

## ğŸ¯ CORE SYSTEM OVERVIEW

This is a **Retrieval-Augmented Generation (RAG)** system for searching Malta commercial law documents. It combines:
- **Semantic search** using OpenAI embeddings
- **Vector database** (ChromaDB) for fast retrieval
- **AI-powered summaries** using GPT-4o-mini
- **Beautiful Streamlit interface** for end users

---

## ğŸ—ï¸ ARCHITECTURE FLOW

```
PDFs â†’ OCR Conversion â†’ Text Files â†’ Document Processor â†’ Chunks â†’ 
Vector Embeddings â†’ ChromaDB â†’ Search Engine â†’ AI Assistant â†’ Streamlit UI
```

---

## ğŸ“‚ CLEAN FILE STRUCTURE (Post-Cleanup)

### **ğŸ”´ CORE APPLICATION FILES (DO NOT DELETE)**

#### Main Application
- `main.py` - **Streamlit web interface** (your main entry point)
- `env` - Environment variables (contains `OPENAI_API_KEY`)

#### Core Processing Pipeline
- `doc_processor.py` - **Document processor**: extracts articles, creates chunks
- `vector_store.py` - **Vector database interface**: ChromaDB + OpenAI embeddings
- `search_engine.py` - **Smart search engine**: query understanding + semantic search
- `ai_assistant.py` - **AI assistant**: generates overviews from retrieved articles
- `debug_logger.py` - **Logging system**: tracks queries and errors

#### Data Files
- `processed_chunks.json` - **Processed chunks** ready for vector DB
- `processing_report.json` - **Processing statistics**
- `malta_commercial_code_text.txt` - **Commercial Code (Cap. 13)** source text

#### Configuration
- `Requirements.txt` - Python dependencies (note: capital R)
- `README.md` - Project documentation

---

### **ğŸŸ¡ DATA DIRECTORIES**

#### ChromaDB Vector Database
```
chroma_db/
  â””â”€â”€ chroma.sqlite3  # Your vector embeddings (DO NOT DELETE)
```

#### OCR Processing
```
ocr/
  â”œâ”€â”€ docling_ocr.py         # OCR conversion script (active)
  â”œâ”€â”€ requirements.txt       # OCR-specific dependencies
  â”œâ”€â”€ README.md             # OCR documentation
  â”œâ”€â”€ input_pdfs/           # âš ï¸ SOURCE PDFs (21 files)
  â”‚   â””â”€â”€ SUBSIDIARY LEGISLATION 386 *.pdf
  â””â”€â”€ output/               # ğŸŸ¢ CONVERTED TEXT FILES (22 files)
      â”œâ”€â”€ Companies Act.txt
      â””â”€â”€ SUBSIDIARY LEGISLATION *.txt
```

#### Debug Logs
```
debug_logs/
  â”œâ”€â”€ ai_assistant.log
  â”œâ”€â”€ doc_processor.log
  â”œâ”€â”€ main_app.log
  â”œâ”€â”€ queries.log
  â”œâ”€â”€ search_engine.log
  â””â”€â”€ vector_store.log
```

---

### **âš ï¸ UNPROCESSED DATA**

#### Legislation Folder (43 PDFs)
```
Legislation/
  â””â”€â”€ *.pdf  # 43 PDF files NOT currently being processed
```

**âš ï¸ IMPORTANT**: These 43 PDFs in the `Legislation/` folder are **NOT** being used by your system. 
- If you want them indexed, move them to `ocr/input_pdfs/` and run OCR conversion
- If they're duplicates or unnecessary, you can delete this folder

---

## ğŸ”„ HOW TO USE THE SYSTEM

### **1. Run the Streamlit App**
```bash
streamlit run main.py
```

### **2. Add New Documents**
To add new legal documents to your RAG system:

1. Place PDF files in `ocr/input_pdfs/`
2. Run OCR conversion:
   ```bash
   cd ocr
   python docling_ocr.py
   ```
3. The system will:
   - Convert PDFs â†’ text files in `ocr/output/`
   - Auto-process on next Streamlit startup
   - Extract articles/regulations
   - Chunk them (3000 tokens with 200 overlap)
   - Generate embeddings
   - Store in ChromaDB

### **3. Query the System**
- Open browser to `http://localhost:8501`
- Enter queries like:
  - "What is a trader?"
  - "Article 477"
  - "company director duties"
  - "bankruptcy procedures"

---

## ğŸ—‚ï¸ WHAT WAS DELETED (Cleanup Summary)

### Folders Removed:
- âŒ `deprecated/` - Old preview/test scripts
- âŒ `test_chunking/` - Chunking quality tests
- âŒ `dashboards/` - Monitoring demos (not part of core RAG)
- âŒ `scripts/` - One-off utility scripts (batch converters, fixers, loaders)
- âŒ `ocr/test_output/` - Test files
- âŒ `ocr/demo_output/` - Demo files
- âŒ `__pycache__/` - Python cache files

### Files Removed:
- âŒ `document_processor.py` - Redundant wrapper
- âŒ `full_workflow.py` - Alternative workflow (not used)
- âŒ `all_processed_chunks.json` - Duplicate chunks file
- âŒ `chunks_dump.txt` - Old chunks dump
- âŒ `whole_article_chunking_demo.json` - Demo file
- âŒ `extracted_statutory_data_sample.json` - Sample extraction
- âŒ `load_chunks_to_db.py` - One-off loader script
- âŒ `reset_and_reprocess.py` - One-off utility
- âŒ `generate_questions.py` - Question generator
- âŒ `statutory_extractor.py` - Unused extractor
- âŒ `commercial_code_sample_questions.md` - Sample questions
- âŒ `DEPLOYMENT_GUIDE.md` - Outdated deployment guide
- âŒ `gcv_service_account.json` - Google Cloud credentials (not used)
- âŒ `ocr/file_bot.py` - Alternative OCR (not used)
- âŒ `ocr/gcv_ocr.py` - Google Cloud Vision OCR (not used)

---

## ğŸ”§ SYSTEM CONFIGURATION

### Required Environment Variables (in `env` file):
```
OPENAI_API_KEY=your_key_here
```

### Key Parameters (in `doc_processor.py`):
- **Chunk size**: 3000 tokens
- **Overlap**: 200 tokens
- **Embedding model**: `text-embedding-3-large` (OpenAI)
- **AI model**: `gpt-4o-mini` (for overviews)

### ChromaDB Collection:
- **Name**: `malta_code_v2`
- **Distance metric**: Cosine similarity

---

## ğŸ“Š CURRENT DATA SOURCES

Your system currently processes:
1. âœ… **Commercial Code (Cap. 13)** - `malta_commercial_code_text.txt`
2. âœ… **Companies Act (Cap. 386)** - `ocr/output/Companies Act.txt`
3. âœ… **Subsidiary Legislation 386.02-386.24** - 20 text files in `ocr/output/`
4. âœ… **Subsidiary Legislation 595.27** - 1 text file

**Total**: 22 legal documents indexed

---

## ğŸš€ NEXT STEPS

### Option 1: Keep Legislation Folder
If those 43 PDFs in `Legislation/` are important:
1. Move them to `ocr/input_pdfs/`
2. Run OCR conversion
3. Restart the system to auto-process

### Option 2: Delete Legislation Folder
If they're duplicates or unnecessary:
```bash
Remove-Item -Recurse -Force Legislation
```

### Recommended: Add .gitignore
Create a `.gitignore` file:
```
__pycache__/
*.pyc
chroma_db/
debug_logs/
.env
env
gcv_service_account.json
processed_chunks.json
processing_report.json
```

---

## ğŸ“ MAINTENANCE

### Clear Database and Reprocess
If you need to rebuild the vector database:
1. Delete `chroma_db/` folder
2. Delete `processed_chunks.json`
3. Restart Streamlit - it will auto-rebuild

### View Logs
Check `debug_logs/` for troubleshooting:
- `queries.log` - User search queries
- `ai_assistant.log` - AI overview generation
- `search_engine.log` - Search results

---

## ğŸ’¡ TIPS

1. **Debug Mode**: Click the âš™ï¸ button 3 times in the Streamlit UI to enable debug mode
2. **Performance**: The system uses ChromaDB's HNSW index for fast similarity search
3. **AI Overviews**: Automatically generated when `enable_ai_overview=True` in `search_engine.py`
4. **Query Understanding**: The system auto-detects intent (definition, procedural, penalty, etc.)

---

**Last Updated**: After major cleanup on October 24, 2025
**System Status**: âœ… Fully functional and optimized


